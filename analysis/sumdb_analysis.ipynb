{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2 rich\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from psycopg2 import sql\n",
    "from rich import print\n",
    "import psycopg2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs a message to the console.\n",
    "    \"\"\"\n",
    "    print(message)\n",
    "    with open(\"sumdb_log.txt\", \"a\") as log_file:\n",
    "        log_file.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(conn, table):\n",
    "    \"\"\"\n",
    "    Fetches and logs the column names of a specified table.\n",
    "    \"\"\"\n",
    "    query = sql.SQL(\"\"\"\n",
    "        SELECT column_name \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_name = %s\n",
    "        ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query, (table,))\n",
    "    columns = cur.fetchall()\n",
    "    cur.close()\n",
    "    return [col[0] for col in columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW OF SUMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = '../inputs'\n",
    "filename = \"input.csv\"\n",
    "topicFileName = \"topics.txt\"\n",
    "\n",
    "inputFilePath = f\"{dirs}/{filename}\"\n",
    "topicFilePath = f\"{dirs}/{topicFileName}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find len\n",
    "with open(topicFilePath, 'r') as file:\n",
    "    true_topics = file.readlines()\n",
    "    true_topics = [topic.strip() for topic in true_topics]\n",
    "    \n",
    "print(f\"Number of true topics: {len(true_topics)}\")\n",
    "print(f\"True topics: {true_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_topic = \"localhost\"  # using localhost for now\n",
    "port = \"5432\"\n",
    "dbname = \"db\"  # internal database name\n",
    "username = \"user\"\n",
    "password = \"password\"\n",
    "\n",
    "table = \"test\"  # Name of table to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=dbname,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    host=db_topic,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "# Format datetime for readability\n",
    "formatted_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "log(f\"[{formatted_datetime}] Connected to database '{dbname}' on {db_topic}:{port} as '{username}'\")\n",
    "\n",
    "# Get and log column names\n",
    "column_names = get_column_names(conn, table)\n",
    "log(\"Column names in '{}' table: {}\".format(table, \", \".join(column_names)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and execute the query\n",
    "query = sql.SQL(\"SELECT chunkstart, chunkend, topic FROM {}\").format(sql.Identifier(table))\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "# conn.close()\n",
    "\n",
    "print(rows[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_chunk_count = defaultdict(int)\n",
    "row_topic_pairs = defaultdict(list)\n",
    "row_topic_count = defaultdict(int)\n",
    "\n",
    "for row in rows:\n",
    "    chunk_start, chunk_end, topic = row\n",
    "\n",
    "    topic_chunk_count[topic] += 1\n",
    "    row_topic_pairs[topic].append((chunk_start, chunk_end))\n",
    "    row_topic_count[topic] += abs(chunk_end - chunk_start) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the count of chunks for each topic\n",
    "log('\\nTOPIC NODE ANALYSIS')\n",
    "for topic, count in topic_chunk_count.items():\n",
    "    log(f\"{topic}: {count} chunks\")\n",
    "\n",
    "log('Chunk Count: ' + str(len(rows)) + ' saved chunks')\n",
    "log(f'Actual topic node: {len(topic_chunk_count)} nodes')\n",
    "log(f'Expected topic node: {len(true_topics)} nodes')\n",
    "log(f'Missing {len(true_topics) - len(topic_chunk_count)} nodes: {set(true_topics) - set(topic_chunk_count.keys())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log('\\nDATA COUNT ANALYSIS')\n",
    "row_topic_pairs = dict(sorted(row_topic_pairs.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "for topic, rows in row_topic_pairs.items():\n",
    "    log(f\"{topic}: {row_topic_count[topic]:,}\")\n",
    "    # log(f\"Pairs: {rows}\")\n",
    "\n",
    "total_rows = sum(row_topic_count.values())\n",
    "log(f'\\nTotal rows: {total_rows:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('\\nCONTINUITY CHECK')\n",
    "no_gap = True\n",
    "for topic, chunks in row_topic_pairs.items():\n",
    "    # Sort the chunks by chunkstart to ensure correct order\n",
    "    chunks.sort(key=lambda x: x[0])\n",
    "    for i in range(len(chunks) - 1):\n",
    "        current_chunk_end = chunks[i][1]\n",
    "        next_chunk_start = chunks[i + 1][0]\n",
    "        # Check if there is a gap\n",
    "        if next_chunk_start != current_chunk_end + 1:\n",
    "            log(f\"Gap found in topic '{topic}' within chunks [{current_chunk_end}, {next_chunk_start}]\")\n",
    "            no_gap = False\n",
    "\n",
    "if no_gap:\n",
    "    log('No gap found!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sql.SQL(\"\"\"\n",
    "    SELECT chunkstart, chunkend, topic, summary \n",
    "    FROM {} \n",
    "    ORDER BY updatedat DESC\n",
    "    LIMIT 3;\n",
    "\"\"\").format(sql.Identifier(table))\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=dbname,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    host=db_topic,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "for row in rows:\n",
    "    log(f\"Chunk range: ({row[0]}, {row[1]})\")\n",
    "    log(f'Topic: {row[2]}')\n",
    "    # log(f\"Chunk content: {row[3][-1000:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logosdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
